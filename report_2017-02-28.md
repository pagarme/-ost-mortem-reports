# Indisponibilidade 28/02/2017

Na última terça-feira (28/02), o serviço de hospedagem da Amazon, conhecido como AWS, sofreu uma falha grave no “Simple Storage Service” (S3) da região US-EAST-1. Os detalhes deste evento foram descritos de forma sucinta e objetiva pela própria Amazon [aqui] (https://aws.amazon.com/message/41926/).

Considerando que a AWS controla hoje 31% do mercado [1] de infraestrutura em nuvem, este evento teve repercussão global, pois afetou grandes provedores de serviços como, por exemplo, Netflix, Snapchat, Medium, Github, ZenDesk, Trello e o próprio Pagar.me.

O objetivo desse artigo é descrever nossa experiência, aprendizado e postura frente ao ocorrido.

## Por que alguns serviços foram afetados e outros não?

A AWS é uma plataforma de serviços em nuvem operando em âmbito global e, para atender a toda essa demanda, ela distribui sua infraestrutura em "regiões" ao redor do mundo. Cada região é totalmente isolada e separada geograficamente das outras. 

No caso do Pagar.me, a maior parte da nossa infraestrutura, incluindo nosso Banco de Dados principal, estava localizada justamente na região afetada (US-EAST-1). Qualquer serviço que estivesse hospedado fora dessa região não sofreu com a instabilidade causada pela falha generalizada.

## O que deu errado?

Por volta de 14:37, horário de Brasília, um dos times do S3 da AWS iniciou um processo interno de manutenção que desencadeou toda a série de falhas nos serviços prestados. Por utilizarmos o serviço de banco de dados relacional da AWS (RDS), contamos com um sistema de redundância que nos oferece a possibilidade de rotação entre diferentes zonas de disponibilidade (Multi-Availability Zones), de acordo com o estado operacional e necessidade de nossas instâncias.

A origem do problema se deu quando a AWS identificou, erroneamente, às 14:54, que o nosso Banco de Dados estava fora do ar. Nesse momento, o mecanismo automático de rotação entre zonas (Multi-AZ) entrou em cena e tentou redirecionar o tráfego do Banco de Dados principal para o de back-up. Infelizmente, toda a estrutura interna de rotação (failover) da AWS depende do S3 (serviço que sofreu a primeira falha). Consequentemente, uma reação em cadeia de falhas se estabeleceu e acabou afetando vários outros serviços oferecidos pela AWS, inclusive o transacional do Pagar.me.

A AWS conseguiu restabelecer o funcionamento do S3 às 18:54 e, a partir desse momento, todos os seus outros serviços começaram a se recuperar. Dado o grande volume de dados e requisições acumulados durante o período da falha, foi necessário um tempo adicional para que tudo voltasse à normalidade. Por conseguinte, o Pagar.me voltou a transacionar às 19:53, momento em que a rotação Multi-AZ foi concluída com sucesso.

## Próximos passos

Essa falha nos proporcionou experiências valiosas que nos ajudarão a identificar e priorizar alternativas que garantam que nossos sistemas e serviços sejam mais resilientes tanto a falhas externas quanto internas.

Esse aprendizado também nos fez concluir que, apesar de possuírem uma alta taxa de disponibilidade e durabilidade, não podemos confiar plenamente nos serviços de redundância oferecidos pela AWS. Com isso, nosso primeiro passo será migrar do RDS para uma infraestrutura independente em que tenhamos mais autonomia, liberdade e controle para monitorar e atuar diretamente em caso de falhas. É importante ressaltar que essa migração nos permitirá, também,  desenvolver uma infraestrutura mais transparente e acessível do que a oferecida atualmente pela AWS. 

Além disso, se considerarmos a liberdade adquirida por meio do passo anterior, o próximo será distribuir essa nova infraestrutura em múltiplas regiões, adicionando, dessa forma, uma camada de proteção a mais no sistema de redundância, de modo a torná-lo ainda mais robusto e resiliente a falhas. 

Por fim, iremos elaborar e adaptar um plano de recuperação de desastres com o objetivo de minimizar o tempo de queda de nossos serviços quando ocorrerem falhas desse porte.

--

**[1].** “How One Little Amazon Error Can Destroy the Internet”. Adam Clark Estes. Disponível em:  http://gizmodo.com/how-one-little-amazon-error-can-destroy-the-internet-1792828399




